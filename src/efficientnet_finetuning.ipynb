{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetV2 Fine-Tuning on the real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all the necessary parameters in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which variant of EfficientNetV2 to train\n",
    "model_size = 's' # s, m, l\n",
    "\n",
    "# Choose which dataset to train on (it's stored in ../storage/datasets)\n",
    "dataset_name = 'tiny'\n",
    "\n",
    "# Choose the output directory for the trained model\n",
    "output_dir = f'../storage/trained_efficientnet/{dataset_name}/{model_size}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, default_collate\n",
    "from torch.optim.lr_scheduler import LambdaLR, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "from torchvision import datasets as dset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l, EfficientNet_V2_S_Weights, EfficientNet_V2_M_Weights, EfficientNet_V2_L_Weights\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from medmnist import DermaMNIST, BloodMNIST\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import gc\n",
    "import psutil\n",
    "import scipy\n",
    "import traceback\n",
    "import csv\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=False) # safe check\n",
    "\n",
    "log_filename = os.path.join(output_dir, 'training.log')\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filemode='w') # filemode='w' to overwrite the log file every time the pipeline is run\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPetsDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/pets', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train85.pth' if split == 'train' else root+'/test15.pth'\n",
    "        self.data = torch.load(self.data_dir)\n",
    "        self.classes = sorted(set(label.item() for _, label in self.data)) # -> 0, 1, 2, ..., 36\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class TinyImageNetDataset(Dataset): # from HuggingFace\n",
    "    def __init__(self, root='../datasets/tiny', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/valid'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        # Convert the 1821 grayscale images to RGB by duplicating channels (colour will remain gray, it's just for consistency)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(dataset_name, image_size, huge_augment, horizontal_flip, random_crop, random_erasing, train=True):\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2470, 0.2435, 0.2616]\n",
    "    elif dataset_name == 'cifar100':\n",
    "        mean = [0.5071, 0.4867, 0.4408]\n",
    "        std = [0.2675, 0.2565, 0.2761]\n",
    "    elif dataset_name == 'pets':\n",
    "        mean = [0.4717, 0.4499, 0.3837]\n",
    "        std = [0.2726, 0.2634, 0.2794]\n",
    "    elif dataset_name == 'cars':\n",
    "        mean = [0.4708, 0.4602, 0.4550]\n",
    "        std = [0.2892, 0.2882, 0.2968]\n",
    "    elif dataset_name == 'food':\n",
    "        mean = [0.5450, 0.4435, 0.3436]\n",
    "        std = [0.2695, 0.2719, 0.2766]\n",
    "    elif dataset_name == 'tiny':\n",
    "        mean = [0.4805, 0.4483, 0.3978]\n",
    "        std = [0.2177, 0.2138, 0.2136]\n",
    "    elif dataset_name == 'dermamnist': \n",
    "        mean = [0.7632, 0.5381, 0.5615]\n",
    "        std = [0.0872, 0.1204, 0.1360]\n",
    "    elif dataset_name == 'bloodmnist':\n",
    "        mean = [0.7961, 0.6596, 0.6964]\n",
    "        std = [0.2139, 0.2464, 0.0903]\n",
    "    else:\n",
    "        raise TypeError(f\"Unknown dataset: {dataset_name}. Supported dataset names are cifar10, cifar100, pets, cars, food, tiny, dermamnist, bloodmnist.\")\n",
    "\n",
    "    transformations = [v2.ToTensor(), v2.Resize(image_size, interpolation=Image.BICUBIC, antialias=True)] # for both train and test sets\n",
    "\n",
    "    if train:\n",
    "        if horizontal_flip:\n",
    "            transformations.append(v2.RandomHorizontalFlip())\n",
    "\n",
    "        if huge_augment == 'trivial_augment':\n",
    "            transformations.append(v2.TrivialAugmentWide())\n",
    "        elif huge_augment == 'auto_augment':\n",
    "            transformations.append(v2.AutoAugment())\n",
    "        elif huge_augment == 'rand_augment':\n",
    "            transformations.append(v2.RandAugment())\n",
    "        elif huge_augment == 'aug_mix':\n",
    "            transformations.append(v2.AugMix())\n",
    "\n",
    "        if random_crop:\n",
    "            padding_fraction = 0.10  # 10% padding\n",
    "            new_padding = int(image_size[0] * padding_fraction)\n",
    "            transformations.append(v2.RandomCrop(image_size[0], padding=new_padding))\n",
    "\n",
    "    transformations.append(v2.Normalize(mean=mean, std=std)) # also for the test set\n",
    "\n",
    "    if train and random_erasing: # i put it after normalization for consistency with the code i used so far\n",
    "        transformations.append(v2.RandomErasing(value='random'))\n",
    "\n",
    "    transform = v2.Compose(transformations)\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_dataset(dataset_name, image_size, huge_augment, horizontal_flip, random_crop, random_erasing):\n",
    "    \n",
    "    logger.info(f'Dataset {dataset_name} loading and processing...')\n",
    "\n",
    "    training_transformations = get_transform(dataset_name, image_size, huge_augment, horizontal_flip, random_crop, random_erasing, train=True)\n",
    "    test_transformations = get_transform(dataset_name, image_size, huge_augment, horizontal_flip, random_crop, random_erasing, train=False)\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        train_data = dset.CIFAR10(root='../datasets/cifar10', train=True, transform=training_transformations, download=True)\n",
    "        test_data = dset.CIFAR10(root='../datasets/cifar10', train=False, transform=test_transformations, download=True)\n",
    "    elif dataset_name == 'cifar100':\n",
    "        train_data = dset.CIFAR100(root='../datasets/cifar100', train=True, transform=training_transformations, download=True)\n",
    "        test_data = dset.CIFAR100(root='../datasets/cifar100', train=False, transform=test_transformations, download=True)\n",
    "    elif dataset_name == 'pets':\n",
    "        train_data = OxfordPetsDataset(root='../datasets/pets', split='train', transform=training_transformations)\n",
    "        test_data = OxfordPetsDataset(root='../datasets/pets', split='test', transform=test_transformations)\n",
    "    elif dataset_name == 'cars':\n",
    "        train_data = dset.StanfordCars(root='../datasets/cars', split='train', transform=training_transformations, download=False) # download does not work for this dataset, it's only for backward compatibility\n",
    "        test_data = dset.StanfordCars(root='../datasets/cars', split='test', transform=test_transformations, download=False) # download does not work for this dataset, it's only for backward compatibility\n",
    "    elif dataset_name == 'food':\n",
    "        train_data = dset.Food101(root='../datasets/food', split='train', transform=training_transformations, download=True)\n",
    "        test_data = dset.Food101(root='../datasets/food', split='test', transform=test_transformations, download=True)\n",
    "    elif dataset_name == 'tiny':\n",
    "        train_data = TinyImageNetDataset(root='../datasets/tiny', split='train', transform=training_transformations)\n",
    "        test_data = TinyImageNetDataset(root='../datasets/tiny', split='valid', transform=test_transformations) # valid is the test set for Tiny ImageNet huggingface dataset\n",
    "    elif dataset_name == 'dermamnist':\n",
    "        train_data = DermaMNIST(root='../datasets/dermamnist', split='train', size=224, as_rgb=True, transform=training_transformations, download=True)\n",
    "        test_data = DermaMNIST(root='../datasets/dermamnist', split='test', size=224, as_rgb=True, transform=test_transformations, download=True)\n",
    "    elif dataset_name == 'bloodmnist':\n",
    "        train_data = BloodMNIST(root='../datasets/bloodmnist', split='train', size=224, as_rgb=True, transform=training_transformations, download=True)\n",
    "        test_data = BloodMNIST(root='../datasets/bloodmnist', split='test', size=224, as_rgb=True, transform=test_transformations, download=True)\n",
    "    else:\n",
    "        raise TypeError(f\"Unknown dataset: {dataset_name}. Supported dataset names are cifar10, cifar100, pets, cars, food, tiny, dermamnist, bloodmnist.\")\n",
    "\n",
    "\n",
    "    logger.info(f'Dataset {dataset_name} loaded and processed!\\n')\n",
    "        \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2 definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEfficientNetV2(nn.Module):\n",
    "    def __init__(self, size, num_classes, pretrained=True):\n",
    "        assert size in ['s', 'm', 'l'] # safe check\n",
    "        super(CustomEfficientNetV2, self).__init__()\n",
    "        \n",
    "        if pretrained:\n",
    "            if size == 's':\n",
    "                weights = EfficientNet_V2_S_Weights.IMAGENET1K_V1\n",
    "                self.model = efficientnet_v2_s(weights=weights)\n",
    "            elif size == 'm':\n",
    "                weights = EfficientNet_V2_M_Weights.IMAGENET1K_V1\n",
    "                self.model = efficientnet_v2_m(weights=weights)\n",
    "            elif size == 'l':\n",
    "                weights = EfficientNet_V2_L_Weights.IMAGENET1K_V1\n",
    "                self.model = efficientnet_v2_l(weights=weights)\n",
    "        else:\n",
    "            if size == 's':\n",
    "                self.model = efficientnet_v2_s(weights=None)\n",
    "            elif size == 'm':\n",
    "                self.model = efficientnet_v2_m(weights=None)\n",
    "            elif size == 'l':\n",
    "                self.model = efficientnet_v2_l(weights=None)\n",
    "        \n",
    "        num_features = self.model.classifier[1].in_features\n",
    "        \n",
    "        self.model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_scheduling(optimizer, scheduler_name=None, warmup=False, warmup_epochs=0, epochs=100):\n",
    "    if scheduler_name is not None:\n",
    "        scheduler_list = []\n",
    "\n",
    "        if warmup:\n",
    "            lr_lambda = lambda epoch: (epoch  / warmup_epochs) + 1e-5\n",
    "            warmup_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "            scheduler_list.append(warmup_scheduler)\n",
    "            milestones = [warmup_epochs]\n",
    "        else:\n",
    "            milestones = []\n",
    "\n",
    "        if scheduler_name == 'CosineAnnealingLR':\n",
    "            scheduler_lr = CosineAnnealingLR(optimizer, T_max=epochs - warmup_epochs, eta_min=0, last_epoch=-1, verbose=False)\n",
    "            scheduler_list.append(scheduler_lr)\n",
    "            \n",
    "        if scheduler_list:\n",
    "            scheduler = SequentialLR(optimizer, schedulers=scheduler_list, milestones=milestones)\n",
    "        else:\n",
    "            scheduler = None\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, \n",
    "    train_dataloader,\n",
    "    dataset_name, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scaler, \n",
    "    mixed_precision, \n",
    "    scheduler,\n",
    "    device,\n",
    "    cutmix_or_mixup\n",
    "):\n",
    "    model.train()\n",
    "    total_loss, correct_predictions, total_samples, nan_encountered = 0.0, 0, 0, False\n",
    "    \n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Squeeze (if not already done in collate_fn) for dermamnist or bloodmnist: from shape [batch_size, 1] to shape [batch_size]\n",
    "        if not cutmix_or_mixup and dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() == 2:\n",
    "            labels = labels.squeeze(1)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=mixed_precision):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if torch.isnan(loss).any():\n",
    "                logger.info(\"==> Encountered NaN value in loss.\")\n",
    "                return 0, 0, True\n",
    "            \n",
    "            scaler.scale(loss).backward() if mixed_precision else loss.backward()\n",
    "            scaler.step(optimizer) if mixed_precision else optimizer.step()\n",
    "            scaler.update() if mixed_precision else None\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, int_predictions = torch.max(outputs, 1)\n",
    "        if cutmix_or_mixup:\n",
    "            _, labels = torch.max(labels, 1)\n",
    "        correct_predictions += (int_predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return average_loss, accuracy, nan_encountered\n",
    "\n",
    "def test_epoch(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    dataset_name,\n",
    "    criterion,\n",
    "    device\n",
    "):\n",
    "    model.eval()\n",
    "    total_loss, correct_predictions, total_samples = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "\n",
    "            # Squeeze only for dermamnist or bloodmnist: from shape [batch_size, 1] to shape [batch_size]\n",
    "            if dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() == 2:\n",
    "                labels = labels.squeeze(1)\n",
    "            elif dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() != 2:\n",
    "                raise ValueError(f\"Medmnist labels have an unexpected shape: {labels.shape}\")\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, int_predictions = torch.max(outputs, 1)\n",
    "            correct_predictions += (int_predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    average_loss = total_loss / len(test_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model_size,\n",
    "    dataset_name,\n",
    "    image_size=(224, 224),\n",
    "    epochs=50,\n",
    "    batch_size=96,\n",
    "    optimizer_name='AdamW',\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.00005,\n",
    "    warmup=False,\n",
    "    warmup_epochs=0,\n",
    "    mixed_precision=True,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    scheduler_name='CosineAnnealingLR',\n",
    "    patience=30,\n",
    "    horizontal_flip=True,\n",
    "    huge_augment='aug_mix',\n",
    "    random_crop=True,\n",
    "    random_erasing=False,\n",
    "    cutmix=False,\n",
    "    mixup=True,\n",
    "    label_smoothing=0.1,\n",
    "):\n",
    "\n",
    "    # Write on the log file the training parameters\n",
    "    logger.info(\"Training Parameters:\\n\")\n",
    "    logger.info(f\"seed: {seed}\")\n",
    "    for param_name, param_value in locals().items(): # locals() returns a dictionary with the current local variables\n",
    "        logger.info(f\"{param_name}: {param_value}\")\n",
    "    logger.info(\"---------------------------------------------------------\\n\")\n",
    "\n",
    "    # Record the start time of the script\n",
    "    start_time_script = time.time()\n",
    "\n",
    "    # Load the real dataset\n",
    "    train_dataset, test_dataset = get_real_dataset(\n",
    "        dataset_name = dataset_name, \n",
    "        image_size = image_size,\n",
    "        horizontal_flip = horizontal_flip, \n",
    "        huge_augment = huge_augment, \n",
    "        random_crop = random_crop, \n",
    "        random_erasing = random_erasing\n",
    "    )\n",
    "\n",
    "    # Get the number of classes of the dataset\n",
    "    if dataset_name == 'dermamnist': # since the medmnist datasets don't have the \"classes\" attribute\n",
    "        num_classes = 7\n",
    "    elif dataset_name == 'bloodmnist':\n",
    "        num_classes = 8\n",
    "    else:\n",
    "        num_classes = len(train_dataset.classes)\n",
    "\n",
    "    # Create the train and test dataloaders\n",
    "    logger.info(\"Creating Train and Test Dataloaders...\")\n",
    "    if cutmix or mixup:\n",
    "        if cutmix and mixup:\n",
    "            advanced_transform = v2.RandomChoice([v2.CutMix(num_classes=num_classes), v2.MixUp(num_classes=num_classes)])\n",
    "        elif cutmix:\n",
    "            advanced_transform = v2.CutMix(num_classes=num_classes)\n",
    "        else:\n",
    "            advanced_transform = v2.MixUp(num_classes=num_classes)\n",
    "        \n",
    "        def collate_fn(batch):\n",
    "            images, labels = default_collate(batch)\n",
    "            if dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() == 2:\n",
    "                labels = labels.squeeze(1)\n",
    "            elif dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() != 2:\n",
    "                raise ValueError(f\"Medmnist labels have an unexpected shape: {labels.shape}\")\n",
    "            return advanced_transform(images, labels)\n",
    "\n",
    "        cutmix_or_mixup = True\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2, collate_fn=collate_fn)\n",
    "    else:\n",
    "        cutmix_or_mixup = False\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "    logger.info(f\"Train and Test Dataloaders created!\\n\")\n",
    "\n",
    "    # Create the classifier model\n",
    "    logger.info(\"Creating the classifier...\")\n",
    "    model = CustomEfficientNetV2(size=model_size, num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    logger.info(\"Classifier created!\\n\")\n",
    "\n",
    "    # Create the scaler, the loss function, the optimizer and the learning rate scheduler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    if optimizer_name == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, nesterov=True, momentum=0.9)\n",
    "    if warmup:\n",
    "        epochs += warmup_epochs\n",
    "    scheduler = learning_rate_scheduling(optimizer, scheduler_name, warmup, warmup_epochs, epochs)\n",
    "\n",
    "    # Initialize the training variables\n",
    "    best_accuracy = 0.0\n",
    "    best_model = None\n",
    "    no_improvement_epochs = 0\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'test_loss': [],\n",
    "        'test_accuracy': [],\n",
    "        'learning_rate': [],\n",
    "        'epoch_time': []\n",
    "    }\n",
    "\n",
    "    # Start the training loop\n",
    "    logger.info(\"The training loop starts now!\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_accuracy, nan_encountered = train_epoch(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            dataset_name=dataset_name,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            mixed_precision=mixed_precision,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            cutmix_or_mixup=cutmix_or_mixup\n",
    "        )\n",
    "        \n",
    "        if nan_encountered:\n",
    "            train_loss, train_accuracy, nan_encountered = train_epoch(\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                dataset_name=dataset_name,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scaler=scaler,\n",
    "                mixed_precision=False,\n",
    "                scheduler=scheduler,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "        test_loss, test_accuracy = test_epoch(\n",
    "            model=model,\n",
    "            test_dataloader=test_dataloader,\n",
    "            dataset_name=dataset_name,\n",
    "            criterion=criterion,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        scheduler_last_lr = scheduler.get_last_lr()[0] if scheduler is not None else learning_rate\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_accuracy'].append(test_accuracy)\n",
    "        history['learning_rate'].append(scheduler_last_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - Train Acc: {train_accuracy:.4f} - \"\n",
    "          f\"Test Loss: {test_loss:.4f} - Test Acc: {test_accuracy:.4f} - \"\n",
    "          f\"LR: {scheduler_last_lr:.6f}\")\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_model = copy.deepcopy(model)\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= patience:\n",
    "                logger.info(f\"Early stopping at epoch {epoch+1} due to no improvement in test accuracy for {patience} consecutive epochs.\")\n",
    "                break\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    metadata = {\n",
    "        'model': best_model,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "    # Save the best model and print its accuracy\n",
    "    torch.save(best_model.state_dict(), os.path.join(output_dir, 'best_model.pth'))\n",
    "    logger.info(f\"Best accuracy: {best_accuracy * 100:.2f} %\\n\")\n",
    "\n",
    "    # Record the duration of the script\n",
    "    end_time_script = time.time()\n",
    "    elapsed_time_script = end_time_script - start_time_script\n",
    "    hours, minutes, seconds = int(elapsed_time_script // 3600), int((elapsed_time_script % 3600) // 60), int(elapsed_time_script % 60)\n",
    "    logger.info(f\"Execution completed in {hours}h {minutes}m {seconds}s.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train(\n",
    "        model_size = model_size,\n",
    "        dataset_name = dataset_name,\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during the training process: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
