{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Mean & Std of the Pixel Values of Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torchvision.datasets as dset\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_from_disk\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "\n",
    "class OxfordPetsDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/oxfordpets', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train85.pth' if split == 'train' else root+'/test15.pth'\n",
    "        self.data = torch.load(self.data_dir)\n",
    "        self.classes = sorted(set(label.item() for _, label in self.data)) # -> 0, 1, 2, ..., 36\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class TinyImageNetDataset(Dataset):  \n",
    "    def __init__(self, root='../datasets/tinyimagenet', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = f\"{root}/train\" if split == 'train' else f\"{root}/valid\"\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class Caltech101Dataset(Dataset):\n",
    "    def __init__(self, root='../datasets/caltech101', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/test'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class ImagewoofDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/imagewoof', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/validation'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Uncomment the dataset you want to compute mean and std for\n",
    "\n",
    "# dataset = dset.CIFAR10(root='../datasets/cifar10', train=True, download=True, transform=transform)\n",
    "# dataset = dset.CIFAR100(root='../datasets/cifar100', train=True, download=True, transform=transform)\n",
    "# dataset = OxfordPetsDataset(root='../datasets/oxfordpets', split='train', transform=transform) \n",
    "# dataset = dset.StanfordCars(root='../datasets/stanfordcars', split='train', download=False, transform=transform)\n",
    "# dataset = dset.Food101(root='../datasets/food101', split='train', download=True, transform=transform)\n",
    "# dataset = TinyImageNetDataset(root='../datasets/tinyimagenet', split='train', transform=transform)\n",
    "# dataset = dset.STL10(root='../datasets/stl10', split='train', download=True, transform=transform)\n",
    "# dataset = dset.Imagenette(root='../datasets/imagenette', split='train', transform=transform, download=True)\n",
    "# dataset = ImagewoofDataset(root='../datasets/imagewoof', split='train', transform=transform)\n",
    "# dataset = Caltech101Dataset(root='../datasets/caltech101', split='train', transform=transform)\n",
    "\n",
    "\n",
    "# Resize to square images, otherwise torch.stack will return error\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "batch_size = 96\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize variables to compute mean and std\n",
    "mean = torch.zeros(3)\n",
    "std = torch.zeros(3)\n",
    "total_images = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting computation of mean and standard deviation...\")\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "\n",
    "    # Log progress every 1000 images\n",
    "    if total_images % 1000 == 0 and total_images != 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {total_images}/{len(dataset)} images. Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    batch_samples = images.size(0)\n",
    "    total_images += batch_samples\n",
    "\n",
    "    # Reshape images to (batch_size, channels, height * width)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "\n",
    "    # Compute mean and std for the batch\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Finished processing {total_images} images in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "# Final computation\n",
    "mean /= total_images\n",
    "std /= total_images\n",
    "\n",
    "# Print results\n",
    "print(f'Number of images: {total_images}')\n",
    "print(f'Mean (R, G, B): {mean[0]:.4f}, {mean[1]:.4f}, {mean[2]:.4f}')\n",
    "print(f'Std (R, G, B): {std[0]:.4f}, {std[1]:.4f}, {std[2]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Mean and Std of the Number of Training Samples per Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk\n",
    "import scipy\n",
    "\n",
    "\n",
    "class OxfordPetsDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/pets', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train85.pth' if split == 'train' else root+'/test15.pth'\n",
    "        self.data = torch.load(self.data_dir)\n",
    "        self.classes = sorted(set(label.item() for _, label in self.data)) # -> 0, 1, 2, ..., 36\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class TinyImageNetDataset(Dataset):  \n",
    "    def __init__(self, root='../datasets/tiny', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = f\"{root}/train\" if split == 'train' else f\"{root}/valid\"\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class Caltech101Dataset(Dataset):\n",
    "    def __init__(self, root='../datasets/caltech101', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/test'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class ImagewoofDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/imagewoof', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/validation'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "# Uncomment the dataset you want to compute mean and std for\n",
    "\n",
    "#dataset = dset.CIFAR10(root='../datasets/cifar10', train=True, download=True, transform=None)\n",
    "#dataset = dset.CIFAR100(root='../datasets/cifar100', train=True, download=True, transform=None)\n",
    "#dataset = OxfordPetsDataset(root='../datasets/pets', split='train', transform=None) \n",
    "dataset = dset.StanfordCars(root='../datasets/cars', split='train', download=False, transform=None)\n",
    "#dataset = dset.Food101(root='../datasets/food', split='train', download=True, transform=None)\n",
    "#dataset = TinyImageNetDataset(root='../datasets/tiny', split='train', transform=None)\n",
    "#dataset = dset.STL10(root='../datasets/stl', split='train', download=True, transform=None)\n",
    "#dataset = dset.Imagenette(root='../datasets/imagenette', split='train', transform=None, download=True)\n",
    "#dataset = Caltech101Dataset(root='../datasets/caltech101', split='train', transform=None)\n",
    "#dataset = ImagewoofDataset(root='../datasets/imagewoof', split='train', transform=None)\n",
    "\n",
    "\n",
    "class_counts = torch.zeros(len(dataset.classes))\n",
    "for _, label in dataset:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "mean = class_counts.mean()\n",
    "std = class_counts.std()\n",
    "\n",
    "print(f\"Mean: {mean:.0f}, Standard deviation: {std:.0f}\")\n",
    "print(f\"Class counts: {class_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save an Hugging Face dataset to your disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Define the path where you want to save the dataset\n",
    "save_path = '../datasets/tinyimagenet'\n",
    "hf_dataset_name = 'zh-plus/tiny-imagenet'\n",
    "\n",
    "# Check if the dataset directory already exists to avoid re-downloading\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    dataset_train = load_dataset(hf_dataset_name, split='train')\n",
    "    dataset_valid = load_dataset(hf_dataset_name, split='valid')\n",
    "\n",
    "    dataset_train.save_to_disk(os.path.join(save_path, 'train'))\n",
    "    dataset_valid.save_to_disk(os.path.join(save_path, 'valid'))\n",
    "\n",
    "    print(f\"HF dataset saved to {save_path}\")\n",
    "else:\n",
    "    print(f\"HF dataset already exists at {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
