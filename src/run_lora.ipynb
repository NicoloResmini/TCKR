{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for running the LoRA script (i.e. Stable Diffusion Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    \"hf_stablediffusion_name\": \"stabilityai/stable-diffusion-2\", # set the name of the hugging face Stable Diffusion model\n",
    "    \"num_train_samples\": 9025, # set the number of training samples (e.g., 9025 is the entire Imagewoof)\n",
    "    \"hf_dataset_name\": \"frgfm/imagewoof\", # set the name of the hugging face dataset\n",
    "    \"hf_image_column\": \"image\", # set the name of the image column in the hugging face dataset\n",
    "    \"prompt_type\": \"className_and_blip2\", # set the prompt type to use (options: \"only_className\", \"className_and_claude\", \"only_blip2\", \"className_and_blip2\")\n",
    "    \"prompts_file\": \"../storage/prompts/imagewoof_blip2_finetuning.txt\", # set the path to the prompts file\n",
    "    \"resolution\": 224, # set the resolution of the images\n",
    "    \"train_batch_size\": 1, # set the batch size\n",
    "    \"gradient_accumulation_steps\": 4, # set the gradient accumulation steps\n",
    "    \"num_train_epochs\": 3, # set the number of training epochs\n",
    "    \"learning_rate\": 1e-4, # set the learning rate\n",
    "    \"mixed_precision\": \"fp16\", # set the mixed precision\n",
    "    \"rank\": 4, # set the LoRA rank\n",
    "    \"seed\": 420, # set the random seed\n",
    "    \"output_dir\": \"../storage/finetuned_SD/imagewoof/sd_2/only_unet/className_and_blip2/1.00\" # set the output directory for the finetuned model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the finetuning script\n",
    "!accelerate launch lora.py \\\n",
    "    --pretrained_model_name_or_path={training_params[\"hf_stablediffusion_name\"]} \\\n",
    "    --dataset_name={training_params[\"hf_dataset_name\"]} --max_train_samples={training_params[\"num_train_samples\"]} --image_column={training_params[\"hf_image_column\"]} \\\n",
    "    --caption_column={training_params[\"prompt_type\"]} --captions_file={training_params[\"prompts_file\"]} \\\n",
    "    --resolution={training_params[\"resolution\"]} --center_crop --random_flip \\\n",
    "    --train_batch_size={training_params[\"train_batch_size\"]} --gradient_accumulation_steps={training_params[\"gradient_accumulation_steps\"]} \\\n",
    "    --num_train_epochs={training_params[\"num_train_epochs\"]} \\\n",
    "    --learning_rate={training_params[\"learning_rate\"]} \\\n",
    "    --mixed_precision={training_params[\"mixed_precision\"]} \\\n",
    "    --rank={training_params[\"rank\"]} \\\n",
    "    --seed={training_params[\"seed\"]} \\\n",
    "    --output_dir={training_params[\"output_dir\"]} \n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "# Write the elapsed time and training parameters to the log file\n",
    "log_file_path = os.path.join(training_params[\"output_dir\"], \"finetuning_parameters.log\")\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    log_file.write(\"Finetuning Parameters:\\n\")\n",
    "    for key, value in training_params.items():\n",
    "        log_file.write(f\"{key}: {value}\\n\")\n",
    "    log_file.write(f\"\\nFinetuning Execution Time: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
