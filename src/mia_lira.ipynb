{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d3bec0",
   "metadata": {},
   "source": [
    "## Membership Inference Attacks with LiRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d224e6",
   "metadata": {},
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d0717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/.cache/configs/'\n",
    "os.environ['TORCH_HOME'] = os.getcwd()+'/.cache/torch'\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.getcwd()+'/.cache/huggingface/hub/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import copy\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, default_collate, ConcatDataset, Subset\n",
    "from torch.optim.lr_scheduler import SequentialLR, CosineAnnealingLR\n",
    "from torchvision import datasets as dset\n",
    "from torchvision.transforms import v2\n",
    "from datasets import load_from_disk\n",
    "from medmnist import DermaMNIST, BloodMNIST\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from thop import profile\n",
    "from itertools import product\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fc49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ec9ae",
   "metadata": {},
   "source": [
    "### Choose a dataset and insert its OFA Network Config (available in the table below) and its number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159468a",
   "metadata": {},
   "source": [
    "| Dataset     | MBNv3 network_config                                                                                                       |\n",
    "|-------------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| cifar10     | 10_4-7-6_4-7-6_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_3-7-4_3-7-4_3-7-4_0-0-0_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |\n",
    "| cifar100    | 10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_3-7-4_3-7-4_3-7-4_0-0-0_4-7-4_4-7-4_4-7-6_4-7-4 |\n",
    "| pets        | 10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |\n",
    "| tiny        | 10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-6_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-6 |\n",
    "| cars        | 10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-3_4-7-4_4-7-4_4-7-4_4-7-4_4-5-6_4-5-4 |\n",
    "| food        | 10_4-7-4_4-7-4_4-7-6_4-7-4_4-7-4_4-7-4_4-7-6_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |\n",
    "| dermamnist  | 10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-6 |\n",
    "| bloodmnist  | 10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |\n",
    "| stl         | 10_4-7-4_4-7-4_4-7-6_4-7-4_4-7-4_4-7-4_4-7-6_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |\n",
    "| imagenette  | 10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_3-7-4_3-7-4_3-7-4_0-0-0_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |\n",
    "| caltech101  | 10_4-7-4_4-7-6_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |\n",
    "| imagewoof   | 10_4-7-4_4-7-4_4-7-4_4-7-6_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d49e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset name\n",
    "dataset_name = \"imagenette\"\n",
    "\n",
    "# Specify the OFA (Once-for-All) configuration string\n",
    "ofa_config = \"10_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_3-7-4_3-7-4_3-7-4_0-0-0_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4_4-7-4\"\n",
    "\n",
    "# Define the number of classes in the dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Print current parameters for verification\n",
    "print(f\"Parameters defined:\\n\"\n",
    "      f\"  Dataset Name:\\t\\t{dataset_name}\\n\"\n",
    "      f\"  Number of Classes:\\t{num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060f817",
   "metadata": {},
   "source": [
    "### Set the paths to the dataset and the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory for model storage\n",
    "main_path = \"../storage/shadow_models/\"\n",
    "\n",
    "# Define output directory for shadow models\n",
    "output_path = main_path + dataset_name\n",
    "\n",
    "# Set path for storing models related to LIRA\n",
    "models_path = output_path + \"/lira/\"\n",
    "\n",
    "# Specify directory containing the dataset files\n",
    "dataset_path = \"../datasets/\" + dataset_name\n",
    "\n",
    "# Print all configured paths for verification\n",
    "print(f\"Configured paths:\\n\"\n",
    "      f\"  Main Path:    {main_path}\\n\"\n",
    "      f\"  Output Path:  {output_path}\\n\"\n",
    "      f\"  Models Path:  {models_path}\\n\"\n",
    "      f\"  Dataset Path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d67f27",
   "metadata": {},
   "source": [
    "### Set the number of shadow models and the challenge size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of shadow models (recommended to use 256 based on literature)\n",
    "num_shadows = 256\n",
    "\n",
    "# Define the number of samples to use during the attack for each set (1000 is ok)\n",
    "challenge_size = 1000\n",
    "\n",
    "# Print current parameters for verification\n",
    "print(f\"Parameters defined:\\n\"\n",
    "      f\"  Number of Shadow Modelss:\\t{num_shadows}\\n\"\n",
    "      f\"  Number of Challenge Samples:\\t{challenge_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4b9d3",
   "metadata": {},
   "source": [
    "### Class definition of some datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c283af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPetsDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/pets', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train85.pth' if split == 'train' else root+'/test15.pth'\n",
    "        self.data = torch.load(self.data_dir)\n",
    "        self.classes = sorted(set(label.item() for _, label in self.data)) # -> 0, 1, 2, ..., 36\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/tiny', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/valid'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[int(index)]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "class Caltech101Dataset(Dataset):\n",
    "    def __init__(self, root='../datasets/caltech101', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/test'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[int(index)]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class ImagewoofDataset(Dataset):\n",
    "    def __init__(self, root='../datasets/imagewoof', split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data_dir = root+'/train' if split == 'train' else root+'/validation'\n",
    "        self.data = load_from_disk(self.data_dir)\n",
    "        self.classes = self.data.features['label'].names\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[int(index)]\n",
    "        img = example['image']\n",
    "        label = example['label']\n",
    "\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c961c",
   "metadata": {},
   "source": [
    "### Definition of some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b637b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(dataset_name, image_size, huge_augment, horizontal_flip, random_crop, random_erasing, train=True):\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2470, 0.2435, 0.2616]\n",
    "    elif dataset_name == 'cifar100':\n",
    "        mean = [0.5071, 0.4867, 0.4408]\n",
    "        std = [0.2675, 0.2565, 0.2761]\n",
    "    elif dataset_name == 'pets':\n",
    "        mean = [0.4717, 0.4499, 0.3837]\n",
    "        std = [0.2726, 0.2634, 0.2794]\n",
    "    elif dataset_name == 'cars':\n",
    "        mean = [0.4708, 0.4602, 0.4550]\n",
    "        std = [0.2892, 0.2882, 0.2968]\n",
    "    elif dataset_name == 'food':\n",
    "        mean = [0.5450, 0.4435, 0.3436]\n",
    "        std = [0.2695, 0.2719, 0.2766]\n",
    "    elif dataset_name == 'tiny':\n",
    "        mean = [0.4805, 0.4483, 0.3978]\n",
    "        std = [0.2177, 0.2138, 0.2136]\n",
    "    elif dataset_name == 'dermamnist': \n",
    "        mean = [0.7632, 0.5381, 0.5615]\n",
    "        std = [0.0872, 0.1204, 0.1360]\n",
    "    elif dataset_name == 'bloodmnist':\n",
    "        mean = [0.7961, 0.6596, 0.6964]\n",
    "        std = [0.2139, 0.2464, 0.0903]\n",
    "    elif dataset_name == 'stl':\n",
    "        mean = [0.4467, 0.4398, 0.4066]\n",
    "        std = [0.2185, 0.2159, 0.2183]\n",
    "    elif dataset_name == 'imagenette':\n",
    "        mean = [0.4625, 0.4580, 0.4295]\n",
    "        std = [0.2351, 0.2287, 0.2372]\n",
    "    elif dataset_name == 'caltech101':\n",
    "        mean = [0.5418, 0.5209, 0.4857]\n",
    "        std = [0.2389, 0.2378, 0.2376]\n",
    "    elif dataset_name == 'imagewoof': \n",
    "        mean = [0.4861, 0.4560, 0.3938]\n",
    "        std = [0.2207, 0.2145, 0.2166]\n",
    "    else:\n",
    "        raise TypeError(f\"Unknown dataset: {dataset_name}.\")\n",
    "\n",
    "    transformations = [v2.ToTensor(), v2.Resize(image_size, interpolation=Image.BICUBIC, antialias=True)] # for both train and test sets\n",
    "\n",
    "    if train:\n",
    "        if horizontal_flip:\n",
    "            transformations.append(v2.RandomHorizontalFlip())\n",
    "\n",
    "        if huge_augment == 'trivial_augment':\n",
    "            transformations.append(v2.TrivialAugmentWide())\n",
    "        elif huge_augment == 'auto_augment':\n",
    "            transformations.append(v2.AutoAugment())\n",
    "        elif huge_augment == 'rand_augment':\n",
    "            transformations.append(v2.RandAugment())\n",
    "        elif huge_augment == 'aug_mix':\n",
    "            transformations.append(v2.AugMix())\n",
    "\n",
    "        if random_crop:\n",
    "            padding_fraction = 0.10  # 10% padding\n",
    "            new_padding = int(image_size[0] * padding_fraction)\n",
    "            transformations.append(v2.RandomCrop(image_size[0], padding=new_padding))\n",
    "\n",
    "    transformations.append(v2.Normalize(mean=mean, std=std)) # also for the test set\n",
    "\n",
    "    if train and random_erasing: # i put it after normalization for consistency with the code i used so far\n",
    "        transformations.append(v2.RandomErasing(value='random'))\n",
    "\n",
    "    transform = v2.Compose(transformations)\n",
    "\n",
    "    return transform\n",
    "\n",
    "def extract_minimal_ofa_network(num_classes, verbose=0):\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print('OFA model loading...')\n",
    "\n",
    "    super_net = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True, verbose=False).eval()\n",
    "    super_net.set_active_subnet(d=1, e=0, ks=3)\n",
    "    model = super_net.get_active_subnet(preserve_weight=True)\n",
    "    \n",
    "    in_features = model.classifier.linear.in_features\n",
    "    model.classifier = nn.Sequential(nn.Linear(in_features, num_classes))\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print('OFA model loaded!\\n')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_ofa_network(config, num_classes, verbose=0):\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print('OFA model loading...')\n",
    "    \n",
    "    split_pieces = config.split(\"_\")\n",
    "    first_el = split_pieces.pop(0)\n",
    "\n",
    "    d = [int(piece.split(\"-\")[0]) for i, piece in enumerate(split_pieces) if i % 4 == 0]\n",
    "    k = [int(piece.split(\"-\")[1]) if piece.split(\"-\")[1] != '0' else 3 for piece in split_pieces]\n",
    "    e = [int(piece.split(\"-\")[2]) if piece.split(\"-\")[2] != '0' else 3 for piece in split_pieces]\n",
    "\n",
    "    super_net_name = 'ofa_supernet_mbv3_w10' if first_el == '10' else 'ofa_supernet_mbv3_w12'\n",
    "    super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True, verbose=False).eval()\n",
    "    super_net.set_active_subnet(d=d, e=e, ks=k)\n",
    "    model = super_net.get_active_subnet(preserve_weight=True)\n",
    "    \n",
    "    in_features = model.classifier.linear.in_features\n",
    "    model.classifier = nn.Sequential(nn.Linear(in_features, num_classes))\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print('OFA model loaded!\\n')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def learning_rate_scheduling(optimizer, scheduler_name=None, warmup=False, warmup_epochs=0, epochs=100):\n",
    "    if scheduler_name is not None:\n",
    "        scheduler_list = []\n",
    "\n",
    "        if warmup:\n",
    "            lr_lambda = lambda epoch: (epoch  / warmup_epochs) + 1e-5\n",
    "            warmup_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "            scheduler_list.append(warmup_scheduler)\n",
    "            milestones = [warmup_epochs]\n",
    "        else:\n",
    "            milestones = []\n",
    "\n",
    "        if scheduler_name == 'CosineAnnealingLR':\n",
    "            scheduler_lr = CosineAnnealingLR(optimizer, T_max=epochs - warmup_epochs, eta_min=0, last_epoch=-1, verbose=False)\n",
    "            scheduler_list.append(scheduler_lr)\n",
    "            \n",
    "        if scheduler_list:\n",
    "            scheduler = SequentialLR(optimizer, schedulers=scheduler_list, milestones=milestones)\n",
    "        else:\n",
    "            scheduler = None\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def train_epoch(\n",
    "    model, \n",
    "    train_dataloader,\n",
    "    dataset_name, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scaler, \n",
    "    mixed_precision, \n",
    "    scheduler,\n",
    "    device,\n",
    "    cutmix_or_mixup\n",
    "):\n",
    "    model.train()\n",
    "    total_loss, correct_predictions, total_samples, nan_encountered = 0.0, 0, 0, False\n",
    "    \n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Squeeze (if not already done in collate_fn) for dermamnist or bloodmnist: from shape [batch_size, 1] to shape [batch_size]\n",
    "        if not cutmix_or_mixup and dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() == 2:\n",
    "            labels = labels.squeeze(1)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=mixed_precision):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if torch.isnan(loss).any():\n",
    "                print(\"==> Encountered NaN value in loss.\")\n",
    "                return 0, 0, True\n",
    "            \n",
    "            scaler.scale(loss).backward() if mixed_precision else loss.backward()\n",
    "            scaler.step(optimizer) if mixed_precision else optimizer.step()\n",
    "            scaler.update() if mixed_precision else None\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, int_predictions = torch.max(outputs, 1)\n",
    "        if cutmix_or_mixup:\n",
    "            _, labels = torch.max(labels, 1)\n",
    "        correct_predictions += (int_predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return average_loss, accuracy, nan_encountered\n",
    "\n",
    "def test_epoch(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    dataset_name,\n",
    "    criterion,\n",
    "    device\n",
    "):\n",
    "    model.eval()\n",
    "    total_loss, correct_predictions, total_samples = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.float().to(device), labels.to(device)\n",
    "\n",
    "            # Squeeze only for dermamnist or bloodmnist: from shape [batch_size, 1] to shape [batch_size]\n",
    "            if dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() == 2:\n",
    "                labels = labels.squeeze(1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, int_predictions = torch.max(outputs, 1)\n",
    "            correct_predictions += (int_predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    average_loss = total_loss / len(test_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "def get_stratified_splits(dataset, labels, train_pct=0.7, val_pct=0.15, test_pct=0.15, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    assert abs(train_pct + val_pct + test_pct - 1.0) < 1e-5, \"Percentages must sum to 1\"\n",
    "\n",
    "    classes = np.unique(labels)\n",
    "    indices_per_split = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "        'test': []\n",
    "    }\n",
    "\n",
    "    for c in classes:\n",
    "        class_indices = np.where(np.array(labels) == c)[0]\n",
    "        n_samples = len(class_indices)\n",
    "\n",
    "        n_train = int(n_samples * train_pct)\n",
    "        n_val = int(n_samples * val_pct)\n",
    "\n",
    "        np.random.shuffle(class_indices)\n",
    "\n",
    "        indices_per_split['train'].extend(class_indices[:n_train])\n",
    "        indices_per_split['val'].extend(class_indices[n_train:n_train+n_val])\n",
    "        indices_per_split['test'].extend(class_indices[n_train+n_val:])\n",
    "\n",
    "    train_set = Subset(dataset, indices_per_split['train']) if train_pct > 0 else None\n",
    "    val_set = Subset(dataset, indices_per_split['val']) if val_pct > 0 else None \n",
    "    test_set = Subset(dataset, indices_per_split['test']) if test_pct > 0 else None\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c3656",
   "metadata": {},
   "source": [
    "### Definition of the shadow models training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3068d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_shadow_models(\n",
    "    dataset_name,\n",
    "    num_shadows=256,\n",
    "    image_size=(224, 224),\n",
    "    epochs=50,\n",
    "    batch_size=96,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.00005,\n",
    "    mixed_precision=True,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    scheduler_name='CosineAnnealingLR',\n",
    "    patience=10,\n",
    "    horizontal_flip=True,\n",
    "    huge_augment='aug_mix',\n",
    "    random_crop=True,\n",
    "    random_erasing=False,\n",
    "    cutmix=False,\n",
    "    mixup=True,\n",
    "    label_smoothing=0.1,\n",
    "    main_path=main_path,\n",
    "    dataset_path=dataset_path\n",
    "):\n",
    "\n",
    "    # Write on the log file the training parameters\n",
    "    print(\"Training Parameters:\\n\")\n",
    "    print(f\"seed: {seed}\")\n",
    "    for param_name, param_value in locals().items(): # locals() returns a dictionary with the current local variables\n",
    "        print(f\"{param_name}: {param_value}\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "\n",
    "    # Record the start time of the script\n",
    "    start_time_script = time.time()\n",
    "    \n",
    "    # Here I am applying the same preprocessing to each set\n",
    "    train_transformations = get_transform(dataset_name, image_size, huge_augment, horizontal_flip, random_crop, random_erasing, train=True)\n",
    "    eval_transformations = get_transform(dataset_name, image_size, huge_augment=None, horizontal_flip=False, random_crop=False, random_erasing=False, train=False)\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        data = dset.CIFAR10(root=dataset_path, train=False, transform=eval_transformations, download=True)\n",
    "    elif dataset_name == 'cifar100':\n",
    "        data = dset.CIFAR100(root=dataset_path, train=False, transform=eval_transformations, download=True)\n",
    "    elif dataset_name == 'pets':\n",
    "        data = OxfordPetsDataset(root=dataset_path, split='test', transform=eval_transformations)\n",
    "    elif dataset_name == 'cars':\n",
    "        data = dset.StanfordCars(root=dataset_path, split='test', transform=eval_transformations, download=False) \n",
    "    elif dataset_name == 'food':\n",
    "        data = dset.Food101(root=dataset_path, split='test', transform=eval_transformations, download=True)\n",
    "    elif dataset_name == 'tiny':\n",
    "        data = TinyImageNetDataset(root=dataset_path, split='valid', transform=eval_transformations)\n",
    "    elif dataset_name == 'dermamnist':\n",
    "        data = DermaMNIST(root=dataset_path, split='test', size=224, as_rgb=True, transform=eval_transformations, download=True)\n",
    "    elif dataset_name == 'bloodmnist':\n",
    "        data = BloodMNIST(root=dataset_path, split='test', size=224, as_rgb=True, transform=eval_transformations, download=True)\n",
    "    elif dataset_name == 'stl':\n",
    "        data = dset.STL10(root=dataset_path, split='test', transform=eval_transformations, download=True)\n",
    "    elif dataset_name == 'imagenette':\n",
    "        data = dset.Imagenette(root=dataset_path, split='val', transform=eval_transformations, download=False) # for this dataset, download=True returns an error if the dataset is already downloaded\n",
    "    elif dataset_name == 'caltech101':\n",
    "        data = Caltech101Dataset(root=dataset_path, split='test', transform=eval_transformations)\n",
    "    elif dataset_name == 'imagewoof':\n",
    "        data = ImagewoofDataset(root=dataset_path, split='validation', transform=eval_transformations)\n",
    "    else:\n",
    "        raise TypeError(f\"Unknown dataset: {dataset_name}.\")\n",
    "\n",
    "    \n",
    "    # Get the number of classes of the dataset\n",
    "    if dataset_name == 'dermamnist': # since the medmnist datasets don't have the \"classes\" attribute\n",
    "        num_classes = 7\n",
    "    elif dataset_name == 'bloodmnist':\n",
    "        num_classes = 8\n",
    "    else:\n",
    "        num_classes = len(data.classes)\n",
    "        \n",
    "    n_splits = num_shadows // 2\n",
    "    \n",
    "    # First, check all existing models\n",
    "    existing_pairs = []\n",
    "    missing_pairs = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        model_in_path = os.path.join(\n",
    "            main_path,\n",
    "            dataset_name,\n",
    "            'lira',\n",
    "            f'shadow_model_in_{i}'\n",
    "        )\n",
    "        model_out_path = os.path.join(\n",
    "            main_path,\n",
    "            dataset_name,\n",
    "            'lira',\n",
    "            f'shadow_model_out_{i}'\n",
    "        )\n",
    "        \n",
    "        model_in_exists = os.path.exists(os.path.join(model_in_path, 'model.pt'))\n",
    "        model_out_exists = os.path.exists(os.path.join(model_out_path, 'model.pt'))\n",
    "        \n",
    "        if model_in_exists and model_out_exists:\n",
    "            existing_pairs.append(i)\n",
    "        else:\n",
    "            missing_pairs.append(i)\n",
    "    \n",
    "    if existing_pairs:\n",
    "        print(f\"Found {len(existing_pairs)} existing complete model pairs\")\n",
    "    if missing_pairs:\n",
    "        print(f\"Need to train {len(missing_pairs)} model pairs\")\n",
    "          \n",
    "\n",
    "    # Process only missing pairs\n",
    "    for i in tqdm(missing_pairs, desc=\"Training shadow models\"):\n",
    "        model_in_path = os.path.join(\n",
    "            main_path,\n",
    "            dataset_name,\n",
    "            'lira',\n",
    "            f'shadow_model_in_{i}'\n",
    "        )\n",
    "        model_out_path = os.path.join(\n",
    "            main_path,\n",
    "            dataset_name,\n",
    "            'lira',\n",
    "            f'shadow_model_out_{i}'\n",
    "        )\n",
    "        \n",
    "    \n",
    "        # Prepare datasets\n",
    "        labels_in = [y for _, y in data]\n",
    "        train_in, val_in, test_in = get_stratified_splits(data, labels_in, train_pct=0.5, val_pct=0.1, test_pct=0.4, seed=seed+i)\n",
    "\n",
    "        train_in.dataset.transform = train_transformations\n",
    "        val_in.dataset.transform = eval_transformations\n",
    "        test_in.dataset.transform = eval_transformations\n",
    "\n",
    "        labels_out = [y for _, y in train_in]\n",
    "        train_out, _, _ = get_stratified_splits(train_in, labels_out, train_pct=0.5, val_pct=0.1, test_pct=0.4, seed=seed+i)\n",
    "        train_out.dataset.transform = train_transformations\n",
    "        \n",
    "        print(\"Creating Train and Test Dataloaders...\")\n",
    "        if cutmix or mixup:\n",
    "            if cutmix and mixup:\n",
    "                advanced_transform = v2.RandomChoice([v2.CutMix(num_classes=num_classes), v2.MixUp(num_classes=num_classes)])\n",
    "            elif cutmix:\n",
    "                advanced_transform = v2.CutMix(num_classes=num_classes)\n",
    "            else:\n",
    "                advanced_transform = v2.MixUp(num_classes=num_classes)\n",
    "\n",
    "            def collate_fn(batch):\n",
    "                images, labels = default_collate(batch)\n",
    "                if dataset_name in ['dermamnist', 'bloodmnist'] and labels.dim() == 2:\n",
    "                    labels = labels.squeeze(1)\n",
    "                return advanced_transform(images, labels)\n",
    "\n",
    "            cutmix_or_mixup = True\n",
    "            train_dataloader_in = DataLoader(train_in, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2, collate_fn=collate_fn)\n",
    "            train_dataloader_out = DataLoader(train_out, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2, collate_fn=collate_fn)\n",
    "        else:\n",
    "            cutmix_or_mixup = False\n",
    "            train_dataloader_in = DataLoader(train_in, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "            train_dataloader_out = DataLoader(train_out, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "        val_dataloader_in = DataLoader(val_in, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "        test_dataloader_in = DataLoader(test_in, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "        print(f\"Train and Test Dataloaders created!\\n\")\n",
    "        \n",
    "   \n",
    "        # Train IN model if needed\n",
    "        if not os.path.exists(os.path.join(model_in_path, 'model.pt')):\n",
    "            os.makedirs(model_in_path, exist_ok=True)\n",
    "            \n",
    "            \n",
    "            # Create the classifier model\n",
    "            print(\"Creating the classifier...\")\n",
    "            model = extract_minimal_ofa_network(num_classes)\n",
    "            model = model.to(device)\n",
    "            print(\"Classifier created!\\n\")            \n",
    "            \n",
    "            # Create the scaler, the loss function, the optimizer and the learning rate scheduler\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            scheduler = learning_rate_scheduling(optimizer, scheduler_name, epochs=epochs)\n",
    "\n",
    "            # Initialize the training variables\n",
    "            best_accuracy = 0.0\n",
    "            best_model = None\n",
    "            no_improvement_epochs = 0\n",
    "            history = {\n",
    "                'epoch': [],\n",
    "                'train_loss': [],\n",
    "                'train_accuracy': [],\n",
    "                'val_loss': [],\n",
    "                'val_accuracy': [],\n",
    "                'learning_rate': [],\n",
    "                'epoch_time': [],\n",
    "            }\n",
    "\n",
    "            \n",
    "            # Start the training loop\n",
    "            print(\"The training loop starts now!\\n\")\n",
    "            for epoch in range(epochs):\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_loss, train_accuracy, nan_encountered = train_epoch(\n",
    "                    model=model,\n",
    "                    train_dataloader=train_dataloader_in,\n",
    "                    dataset_name=dataset_name,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    scaler=scaler,\n",
    "                    mixed_precision=mixed_precision,\n",
    "                    scheduler=scheduler,\n",
    "                    device=device,\n",
    "                    cutmix_or_mixup=cutmix_or_mixup\n",
    "                )\n",
    "\n",
    "                if nan_encountered:\n",
    "                    train_loss, train_accuracy, nan_encountered = train_epoch(\n",
    "                        model=model,\n",
    "                        train_dataloader=train_dataloader_in,\n",
    "                        dataset_name=dataset_name,\n",
    "                        criterion=criterion,\n",
    "                        optimizer=optimizer,\n",
    "                        scaler=scaler,\n",
    "                        mixed_precision=False,\n",
    "                        scheduler=scheduler,\n",
    "                        device=device\n",
    "                    )\n",
    "\n",
    "                val_loss, val_accuracy = test_epoch(\n",
    "                    model=model,\n",
    "                    test_dataloader=val_dataloader_in,\n",
    "                    dataset_name=dataset_name,\n",
    "                    criterion=criterion,\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                scheduler_last_lr = scheduler.get_last_lr()[0] if scheduler is not None else learning_rate\n",
    "                epoch_time = time.time() - start_time\n",
    "\n",
    "                history['epoch'].append(epoch + 1)\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['train_accuracy'].append(train_accuracy)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_accuracy'].append(val_accuracy)\n",
    "                history['learning_rate'].append(scheduler_last_lr)\n",
    "                history['epoch_time'].append(epoch_time)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s - \"\n",
    "                  f\"Train Loss: {train_loss:.4f} - Train Acc: {train_accuracy:.4f} - \"\n",
    "                  f\"Val Loss: {val_loss:.4f} - Test Acc: {val_accuracy:.4f} - \"\n",
    "                  f\"LR: {scheduler_last_lr:.6f}\")\n",
    "\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    no_improvement_epochs = 0\n",
    "                else:\n",
    "                    no_improvement_epochs += 1\n",
    "                    if no_improvement_epochs >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1} due to no improvement in test accuracy for {patience} consecutive epochs.\")\n",
    "                        break\n",
    "\n",
    "            \n",
    "            torch.save(best_model, os.path.join(model_in_path, 'model.pt'))\n",
    "            \n",
    "            # Record the duration of the script\n",
    "            end_time_script = time.time()\n",
    "            elapsed_time_script = end_time_script - start_time_script\n",
    "            hours, minutes, seconds = int(elapsed_time_script // 3600), int((elapsed_time_script % 3600) // 60), int(elapsed_time_script % 60)\n",
    "            print(f\"Execution completed in {hours}h {minutes}m {seconds}s.\\n\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del model, best_model\n",
    "            gc.collect()\n",
    "            \n",
    "         \n",
    "        # Train OUT model if needed\n",
    "        if not os.path.exists(os.path.join(model_out_path, 'model.pt')):\n",
    "            os.makedirs(model_out_path, exist_ok=True) \n",
    "            \n",
    "            \n",
    "            # Create the classifier model\n",
    "            print(\"Creating the classifier...\")\n",
    "            model = extract_minimal_ofa_network(num_classes)\n",
    "            model = model.to(device)\n",
    "            print(\"Classifier created!\\n\")            \n",
    "            \n",
    "            # Create the scaler, the loss function, the optimizer and the learning rate scheduler\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            scheduler = learning_rate_scheduling(optimizer, scheduler_name, epochs=epochs)\n",
    "\n",
    "            # Initialize the training variables\n",
    "            best_accuracy = 0.0\n",
    "            best_model = None\n",
    "            no_improvement_epochs = 0\n",
    "            history = {\n",
    "                'epoch': [],\n",
    "                'train_loss': [],\n",
    "                'train_accuracy': [],\n",
    "                'val_loss': [],\n",
    "                'val_accuracy': [],\n",
    "                'learning_rate': [],\n",
    "                'epoch_time': [],\n",
    "            }\n",
    "\n",
    "            \n",
    "            # Start the training loop\n",
    "            print(\"The training loop starts now!\\n\")\n",
    "            for epoch in range(epochs):\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_loss, train_accuracy, nan_encountered = train_epoch(\n",
    "                    model=model,\n",
    "                    train_dataloader=train_dataloader_out,\n",
    "                    dataset_name=dataset_name,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    scaler=scaler,\n",
    "                    mixed_precision=mixed_precision,\n",
    "                    scheduler=scheduler,\n",
    "                    device=device,\n",
    "                    cutmix_or_mixup=cutmix_or_mixup\n",
    "                )\n",
    "\n",
    "                if nan_encountered:\n",
    "                    train_loss, train_accuracy, nan_encountered = train_epoch(\n",
    "                        model=model,\n",
    "                        train_dataloader=train_dataloader_out,\n",
    "                        dataset_name=dataset_name,\n",
    "                        criterion=criterion,\n",
    "                        optimizer=optimizer,\n",
    "                        scaler=scaler,\n",
    "                        mixed_precision=False,\n",
    "                        scheduler=scheduler,\n",
    "                        device=device\n",
    "                    )\n",
    "\n",
    "                val_loss, val_accuracy = test_epoch(\n",
    "                    model=model,\n",
    "                    test_dataloader=val_dataloader_in,\n",
    "                    dataset_name=dataset_name,\n",
    "                    criterion=criterion,\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                scheduler_last_lr = scheduler.get_last_lr()[0] if scheduler is not None else learning_rate\n",
    "                epoch_time = time.time() - start_time\n",
    "\n",
    "                history['epoch'].append(epoch + 1)\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['train_accuracy'].append(train_accuracy)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_accuracy'].append(val_accuracy)\n",
    "                history['learning_rate'].append(scheduler_last_lr)\n",
    "                history['epoch_time'].append(epoch_time)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s - \"\n",
    "                  f\"Train Loss: {train_loss:.4f} - Train Acc: {train_accuracy:.4f} - \"\n",
    "                  f\"Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.4f} - \"\n",
    "                  f\"LR: {scheduler_last_lr:.6f}\")\n",
    "\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    no_improvement_epochs = 0\n",
    "                else:\n",
    "                    no_improvement_epochs += 1\n",
    "                    if no_improvement_epochs >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1} due to no improvement in test accuracy for {patience} consecutive epochs.\")\n",
    "                        break\n",
    "\n",
    "            \n",
    "            torch.save(best_model, os.path.join(model_out_path, 'model.pt'))\n",
    "            \n",
    "            # Record the duration of the script\n",
    "            end_time_script = time.time()\n",
    "            elapsed_time_script = end_time_script - start_time_script\n",
    "            hours, minutes, seconds = int(elapsed_time_script // 3600), int((elapsed_time_script % 3600) // 60), int(elapsed_time_script % 60)\n",
    "            print(f\"Execution completed in {hours}h {minutes}m {seconds}s.\\n\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del model, best_model\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"Shadow model computation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba137e",
   "metadata": {},
   "source": [
    "### Set the batch size for the training of the shadow models (which is done only once per dataset) and train!\n",
    "\n",
    "Note: It checks automatically if some/all shadow models are already trained, so there is no need to comment the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff3bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_shadow_models(dataset_name=dataset_name, num_shadows=num_shadows, batch_size=1928)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff725b",
   "metadata": {},
   "source": [
    "### Definition of LiRA and related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bb3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_logit(probs, y_true, eps=1e-10):\n",
    "    \"\"\"\n",
    "    φstable = log(f(x)_y) - log(Σ_{y'≠y} f(x)_{y'})\n",
    "    \"\"\"\n",
    "    if dataset_name in ['dermamnist', 'bloodmnist']:\n",
    "        y_true = y_true.squeeze()\n",
    "    y_true = y_true.astype(int)\n",
    "    probs = probs.astype(np.float64)\n",
    "    probs = np.clip(probs, eps, 1 - eps)\n",
    "    log_p = np.log(probs)\n",
    "    n_samples, n_classes = probs.shape\n",
    "    log_f_y = log_p[np.arange(n_samples), y_true][:, np.newaxis]\n",
    "    mask = np.ones_like(probs, dtype=bool)\n",
    "    mask[np.arange(n_samples), y_true] = False\n",
    "    probs_others = probs[mask].reshape(n_samples, n_classes - 1)\n",
    "    sum_others = probs_others.sum(axis=1, keepdims=True)\n",
    "    sum_others = np.maximum(sum_others, eps)\n",
    "    log_sum_others = np.log(sum_others)\n",
    "    return log_f_y - log_sum_others\n",
    "\n",
    "def unstable_logit(probs, y_true, eps=1e-10):\n",
    "    \"\"\"\n",
    "    φunstable = log(f(x)_y) - log(1 - f(x)_y)\n",
    "    \"\"\"\n",
    "    if dataset_name in ['dermamnist', 'bloodmnist']:\n",
    "        y_true = y_true.squeeze()\n",
    "    y_true = y_true.astype(int)\n",
    "    probs = np.clip(probs, eps, 1 - eps)\n",
    "    probs = probs.astype(np.float64)\n",
    "    f_y = probs[np.arange(len(probs)), y_true][:, np.newaxis]\n",
    "    return np.log(f_y) - np.log(1 - f_y)\n",
    "\n",
    "def logits_to_score(logits, y_true):\n",
    "    \"\"\"\n",
    "    Compute score directly from pre-softmax logits:\n",
    "    z(x)_y - max_{y'≠y} z(x)_{y'}\n",
    "    \"\"\"\n",
    "    if dataset_name in ['dermamnist', 'bloodmnist']:\n",
    "        y_true = y_true.squeeze()\n",
    "    y_true = y_true.astype(int)\n",
    "    n_samples, n_classes = logits.shape\n",
    "    z_y = logits[np.arange(n_samples), y_true][:, np.newaxis]\n",
    "    logits_excl_true = logits.copy()\n",
    "    logits_excl_true[np.arange(n_samples), y_true] = -np.inf\n",
    "    max_z_others = np.max(logits_excl_true, axis=1, keepdims=True)\n",
    "    return z_y - max_z_others\n",
    "\n",
    "def gaussian_likelihood_ratio(x, mu_in, sigma_in, mu_out, sigma_out):\n",
    "    \"\"\"\n",
    "    Compute log likelihood ratio between two Gaussian distributions in a numerically stable way\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    sigma_in = np.maximum(sigma_in, eps)\n",
    "    sigma_out = np.maximum(sigma_out, eps)\n",
    "    log_lr = (\n",
    "        -0.5 * np.log(2 * np.pi * sigma_in**2)\n",
    "        - 0.5 * ((x - mu_in)**2) / sigma_in**2\n",
    "        + 0.5 * np.log(2 * np.pi * sigma_out**2)\n",
    "        + 0.5 * ((x - mu_out)**2) / sigma_out**2\n",
    "    )\n",
    "    return np.sum(log_lr, axis=1)\n",
    "\n",
    "def one_sided_test(x, mu_out, sigma_out):\n",
    "    \"\"\"\n",
    "    One-sided test for the offline variant (equation 4 from the paper)\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    sigma_out = np.maximum(sigma_out, eps)\n",
    "    return -0.5 * np.sum(((x - mu_out)**2) / sigma_out**2, axis=1)\n",
    "\n",
    "class LiRA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_shadows=256,\n",
    "        attack_type='online',\n",
    "        score_type='logit_stable',\n",
    "        use_global_variance=True,\n",
    "        debug=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize LiRA attack\n",
    "\n",
    "        Args:\n",
    "            n_shadows: number of shadow models (default 256)\n",
    "            attack_type: 'online' or 'offline' (default 'online')\n",
    "            score_type: 'logit_stable', 'logit_unstable' or 'logits' (default 'logit_stable')\n",
    "            use_global_variance: if True use global variance for all examples (default True)\n",
    "            debug: if True enable debug prints (default False)\n",
    "        \"\"\"\n",
    "        self.n_shadows = n_shadows\n",
    "        self.attack_type = attack_type\n",
    "        self.score_type = score_type\n",
    "        self.use_global_variance = use_global_variance\n",
    "        self.debug = debug\n",
    "\n",
    "    def compute_scores(self, preds, y_true):\n",
    "        if self.score_type == 'logit_stable':\n",
    "            return stable_logit(preds, y_true)\n",
    "        elif self.score_type == 'logit_unstable':\n",
    "            return unstable_logit(preds, y_true)\n",
    "        elif self.score_type == 'logits':\n",
    "            return logits_to_score(preds, y_true)\n",
    "        else:\n",
    "            raise ValueError(f\"Score type {self.score_type} not supported\")\n",
    "\n",
    "    def fit_predict(self, shadow_preds_in, shadow_preds_out, target_preds, y_challenge):\n",
    "        shadow_preds_in = shadow_preds_in.astype(np.float64)\n",
    "        shadow_preds_out = shadow_preds_out.astype(np.float64)\n",
    "        target_preds = target_preds.astype(np.float64)\n",
    "        y_challenge = y_challenge.astype(int)\n",
    "        \n",
    "        # 1. Compute scores for shadow OUT\n",
    "        scores_out = np.array([self.compute_scores(p, y_challenge) for p in shadow_preds_out])\n",
    "        target_scores = self.compute_scores(target_preds, y_challenge)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Scores OUT shape:\", scores_out.shape)\n",
    "            print(\"Scores OUT dtype:\", scores_out.dtype)\n",
    "            print(\"Scores OUT sample values:\", scores_out[0:1, 0:5])\n",
    "            print(\"Target scores shape:\", target_scores.shape)\n",
    "            print(\"Target scores dtype:\", target_scores.dtype)\n",
    "            print(\"Target scores sample values:\", target_scores[0:5])\n",
    "\n",
    "        # 3. Estimate Gaussian parameters for OUT\n",
    "        if self.use_global_variance:\n",
    "            mu_out = np.mean(scores_out, axis=(0,1))\n",
    "            var_out = np.var(scores_out.astype(np.float64), axis=(0,1), ddof=1)\n",
    "            var_out = np.maximum(np.nanmean(var_out), np.finfo(float).tiny)\n",
    "        else:\n",
    "            mu_out = np.mean(scores_out, axis=0)\n",
    "            var_out = np.maximum(np.var(scores_out, axis=0), np.finfo(float).tiny)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"mu_out:\", mu_out)\n",
    "            print(\"var_out:\", var_out)\n",
    "\n",
    "        if self.attack_type == 'online':\n",
    "            # Compute scores for shadow IN\n",
    "            scores_in = np.array([self.compute_scores(p, y_challenge) for p in shadow_preds_in])\n",
    "            \n",
    "            if self.debug:\n",
    "                print(\"Scores IN shape:\", scores_in.shape)\n",
    "                print(\"Scores IN dtype:\", scores_in.dtype)\n",
    "                print(\"Scores IN sample values:\", scores_in[0:1, 0:5])\n",
    "\n",
    "            # Estimate Gaussian parameters for IN\n",
    "            if self.use_global_variance:\n",
    "                mu_in = np.mean(scores_in, axis=(0,1))\n",
    "                var_in = np.var(scores_in.astype(np.float64), axis=(0,1), ddof=1)\n",
    "                var_in = np.maximum(np.nanmean(var_in), np.finfo(float).tiny)\n",
    "            else:\n",
    "                mu_in = np.mean(scores_in, axis=0)\n",
    "                var_in = np.maximum(np.var(scores_in, axis=0), np.finfo(float).tiny)\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"mu_in:\", mu_in)\n",
    "                print(\"var_in:\", var_in)\n",
    "\n",
    "            # Compute likelihood ratio\n",
    "            lr = gaussian_likelihood_ratio(\n",
    "                target_scores, \n",
    "                mu_in, np.sqrt(var_in),\n",
    "                mu_out, np.sqrt(var_out)\n",
    "            )\n",
    "        else:\n",
    "            # Offline attack\n",
    "            lr = one_sided_test(\n",
    "                target_scores,\n",
    "                mu_out, np.sqrt(var_out)\n",
    "            )\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Likelihood ratios (before clipping):\", lr)\n",
    "\n",
    "        # Handle extreme numerical values\n",
    "        lr = np.clip(lr, -1e10, 1e10)\n",
    "        mask = ~np.isnan(lr)\n",
    "        lr = lr[mask]\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Likelihood ratios (after clipping):\", lr)\n",
    "            print(\"Number of valid likelihood ratios:\", len(lr))\n",
    "\n",
    "        stats = {\n",
    "            'mu_out': mu_out,\n",
    "            'var_out': var_out\n",
    "        }\n",
    "\n",
    "        if self.attack_type == 'online':\n",
    "            stats.update({\n",
    "                'mu_in': mu_in,\n",
    "                'var_in': var_in\n",
    "            })\n",
    "\n",
    "        return lr, stats, mask\n",
    "\n",
    "    def evaluate(self, likelihood_ratios, y_membership):\n",
    "        \"\"\"\n",
    "        Compute evaluation metrics\n",
    "        \"\"\"\n",
    "        fpr, tpr, _ = roc_curve(y_membership, likelihood_ratios)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "\n",
    "        # TPR at specific FPR\n",
    "        fpr_thresholds = [0.001, 0.01, 0.1]\n",
    "        tpr_at_fpr = {}\n",
    "        for fpr_threshold in fpr_thresholds:\n",
    "            idx = np.searchsorted(fpr, fpr_threshold)\n",
    "            if idx < len(tpr):\n",
    "                tpr_at_fpr[fpr_threshold] = tpr[idx]\n",
    "            else:\n",
    "                tpr_at_fpr[fpr_threshold] = 0.0\n",
    "\n",
    "        return {\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'auc': auc_score,\n",
    "            'tpr_at_fpr': tpr_at_fpr\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4706a",
   "metadata": {},
   "source": [
    "### Challenge data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c2a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_challenge_data(data_in, data_out, challenge_size=100, seed=42):\n",
    "    \"\"\"Challenge data con campionamento efficiente.\"\"\"\n",
    "    # Sample challenge indices\n",
    "    idx_in = np.random.RandomState(seed).choice(len(data_in), challenge_size, replace=False)\n",
    "    idx_out = np.random.RandomState(seed).choice(len(data_out), challenge_size, replace=False)\n",
    "\n",
    "    # Extract only necessary data\n",
    "    X_challenge = np.concatenate([[data_in[i][0] for i in idx_in], \n",
    "                               [data_out[i][0] for i in idx_out]])\n",
    "    y_challenge = np.concatenate([[data_in[i][1] for i in idx_in], \n",
    "                               [data_out[i][1] for i in idx_out]])\n",
    "\n",
    "    y_membership = np.zeros(2*challenge_size)\n",
    "    y_membership[:challenge_size] = 1\n",
    "\n",
    "    return X_challenge, y_challenge, y_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa766d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transformations = get_transform(dataset_name, (224,224), huge_augment=None, horizontal_flip=False, random_crop=False, random_erasing=False, train=False)\n",
    "\n",
    "if dataset_name == 'cifar10':\n",
    "    data_train = dset.CIFAR10(root=dataset_path, train=True, transform=eval_transformations, download=True)\n",
    "    data_test = dset.CIFAR10(root=dataset_path, train=False, transform=eval_transformations, download=True)\n",
    "elif dataset_name == 'cifar100':\n",
    "    data_train = dset.CIFAR100(root=dataset_path, train=True, transform=eval_transformations, download=True)\n",
    "    data_test = dset.CIFAR100(root=dataset_path, train=False, transform=eval_transformations, download=True)\n",
    "elif dataset_name == 'pets':\n",
    "    data_train = OxfordPetsDataset(root=dataset_path, split='train', transform=eval_transformations)\n",
    "    data_test = OxfordPetsDataset(root=dataset_path, split='test', transform=eval_transformations)\n",
    "elif dataset_name == 'cars':\n",
    "    data_train = dset.StanfordCars(root=dataset_path, split='train', transform=eval_transformations, download=False)\n",
    "    data_test = dset.StanfordCars(root=dataset_path, split='test', transform=eval_transformations, download=False)\n",
    "elif dataset_name == 'food':\n",
    "    data_train = dset.Food101(root=dataset_path, split='train', transform=eval_transformations, download=True)\n",
    "    data_test = dset.Food101(root=dataset_path, split='test', transform=eval_transformations, download=True)\n",
    "elif dataset_name == 'tiny':\n",
    "    data_train = TinyImageNetDataset(root=dataset_path, split='train', transform=eval_transformations)\n",
    "    data_test = TinyImageNetDataset(root=dataset_path, split='valid', transform=eval_transformations)\n",
    "elif dataset_name == 'dermamnist':\n",
    "    data_train = DermaMNIST(root=dataset_path, split='train', size=224, as_rgb=True, transform=eval_transformations, download=True)\n",
    "    data_test = DermaMNIST(root=dataset_path, split='test', size=224, as_rgb=True, transform=eval_transformations, download=True)\n",
    "elif dataset_name == 'bloodmnist':\n",
    "    data_train = BloodMNIST(root=dataset_path, split='train', size=224, as_rgb=True, transform=eval_transformations, download=True)\n",
    "    data_test = BloodMNIST(root=dataset_path, split='test', size=224, as_rgb=True, transform=eval_transformations, download=True)\n",
    "elif dataset_name == 'stl':\n",
    "    data_train = dset.STL10(root=dataset_path, split='train', transform=eval_transformations, download=True)\n",
    "    data_test = dset.STL10(root=dataset_path, split='test', transform=eval_transformations, download=True)\n",
    "elif dataset_name == 'imagenette':\n",
    "    data_train = dset.Imagenette(root=dataset_path, split='train', transform=eval_transformations, download=False)\n",
    "    data_test = dset.Imagenette(root=dataset_path, split='val', transform=eval_transformations, download=False)\n",
    "elif dataset_name == 'caltech101':\n",
    "    data_train = Caltech101Dataset(root=dataset_path, split='train', transform=eval_transformations)\n",
    "    data_test = Caltech101Dataset(root=dataset_path, split='test', transform=eval_transformations)\n",
    "elif dataset_name == 'imagewoof':\n",
    "    data_train = ImagewoofDataset(root=dataset_path, split='train', transform=eval_transformations)\n",
    "    data_test = ImagewoofDataset(root=dataset_path, split='validation', transform=eval_transformations)\n",
    "else:\n",
    "    raise TypeError(f\"Unknown dataset: {dataset_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2370f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_challenge, y_challenge, y_membership = prepare_challenge_data(\n",
    "    data_in=data_train,    \n",
    "    data_out=data_test,    \n",
    "    challenge_size=challenge_size,   \n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de904a9",
   "metadata": {},
   "source": [
    "### Set the path to the model to attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path of the model to attack\n",
    "model_to_attack = \"../storage/trained_synthetic_classifiers/20250201_0051.pth\"\n",
    "                                \n",
    "# Print current parameters for verification\n",
    "print(f\"Parameters defined:\\n\"\n",
    "      f\"  Model to Attack:\\t{model_to_attack}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f41cbc",
   "metadata": {},
   "source": [
    "### Load the model to attack and the challenge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f347470",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = extract_ofa_network(ofa_config, num_classes)\n",
    "target_model.load_state_dict(torch.load(model_to_attack))\n",
    "target_model.to(device)\n",
    "target_model.eval()\n",
    "\n",
    "target_preds = []\n",
    "with torch.no_grad():\n",
    "    for images in DataLoader(X_challenge, batch_size=512):\n",
    "        pred = target_model(images.to(device)).cpu() \n",
    "        target_preds.append(pred)\n",
    "        \n",
    "target_preds = torch.cat(target_preds, dim=0).numpy()\n",
    "target_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b95eed",
   "metadata": {},
   "source": [
    "### Get the shadows' predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shadow_predictions(models_path, dataset, N_SHADOW_MODELS, device='cuda'):\n",
    "    shadow_preds = {'in': [], 'out': []}\n",
    "\n",
    "    for i in tqdm(range(N_SHADOW_MODELS // 2)):\n",
    "        for model_type in ['in', 'out']:\n",
    "            model = torch.load(f'{models_path}/shadow_model_{model_type}_{i}/model.pt')\n",
    "            model.eval()\n",
    "\n",
    "            predictions = []\n",
    "            with torch.no_grad():\n",
    "                for images in DataLoader(dataset, batch_size=512):\n",
    "                    pred = model(images.to(device)).cpu()\n",
    "                    predictions.append(pred)\n",
    "\n",
    "            shadow_preds[model_type].append(torch.cat(predictions).numpy())\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return np.array(shadow_preds['in']), np.array(shadow_preds['out'])\n",
    "\n",
    "shadow_preds_in, shadow_preds_out = get_shadow_predictions(models_path, X_challenge, num_shadows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14b1ee",
   "metadata": {},
   "source": [
    "### Test all the combinations and plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_in = np.isnan(shadow_preds_in).any(axis=(1, 2))\n",
    "mask_out = np.isnan(shadow_preds_out).any(axis=(1, 2))\n",
    "mask_combined = mask_in | mask_out\n",
    "mask_keep = ~mask_combined\n",
    "filtered_shadow_preds_in = shadow_preds_in[mask_keep]\n",
    "filtered_shadow_preds_out = shadow_preds_out[mask_keep]\n",
    "filtered_shadow_preds_in.shape, filtered_shadow_preds_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_combinations(N_SHADOW_MODELS, filtered_shadow_preds_in, filtered_shadow_preds_out, \n",
    "                         target_preds, y_challenge, y_membership):\n",
    "    # Define all possible combinations\n",
    "    attack_types = ['offline', 'online']\n",
    "    score_types = ['logit_stable', 'logit_unstable', 'logits']\n",
    "    variance_options = [True, False]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Test all combinations\n",
    "    for attack_type, score_type, use_global_variance in product(attack_types, score_types, variance_options):\n",
    "        # Initialize the attack with the current combination\n",
    "        attack = LiRA(\n",
    "            n_shadows=N_SHADOW_MODELS,\n",
    "            attack_type=attack_type,\n",
    "            score_type=score_type,\n",
    "            use_global_variance=use_global_variance\n",
    "        )\n",
    "\n",
    "        # Execute the attack\n",
    "        likelihood_ratios, stats, mask = attack.fit_predict(\n",
    "            shadow_preds_in=filtered_shadow_preds_in,\n",
    "            shadow_preds_out=filtered_shadow_preds_out,\n",
    "            target_preds=target_preds,\n",
    "            y_challenge=y_challenge\n",
    "        )\n",
    "\n",
    "        # Evaluate the attack\n",
    "        metrics = attack.evaluate(likelihood_ratios, y_membership[mask])\n",
    "\n",
    "        # Save the results\n",
    "        results.append({\n",
    "            'attack_type': attack_type,\n",
    "            'score_type': score_type,\n",
    "            'use_global_variance': use_global_variance,\n",
    "            'auc': metrics['auc'],\n",
    "            'metrics': metrics\n",
    "        })\n",
    "\n",
    "        print(f\"Combinazione testata - Attack: {attack_type}, Score: {score_type}, \"\n",
    "              f\"Global Variance: {use_global_variance}, AUC: {metrics['auc']:.4f}\")\n",
    "    \n",
    "    # Find the best result\n",
    "    best_result = max(results, key=lambda x: x['auc'])\n",
    "    \n",
    "    print(\"\\nMigliore combinazione trovata:\")\n",
    "    print(f\"Attack Type: {best_result['attack_type']}\")\n",
    "    print(f\"Score Type: {best_result['score_type']}\")\n",
    "    print(f\"Global Variance: {best_result['use_global_variance']}\")\n",
    "    print(f\"AUC: {best_result['auc']:.4f}\")\n",
    "    \n",
    "    # Create plots for the best result\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Standard plot\n",
    "    ax1.plot(best_result['metrics']['fpr'], best_result['metrics']['tpr'], \n",
    "             color='tab:blue', lw=2)\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title(f'ROC Curve (AUC = {best_result[\"auc\"]:.3f})')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log scale plot\n",
    "    ax2.plot(best_result['metrics']['fpr'], best_result['metrics']['tpr'], \n",
    "             color='tab:blue', lw=2)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('ROC Curve (Log Scale)')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a DataFrame with all the results for easier analysis\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Attack Type': r['attack_type'],\n",
    "            'Score Type': r['score_type'],\n",
    "            'Global Variance': r['use_global_variance'],\n",
    "            'AUC': r['auc']\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nTutti i risultati ordinati per AUC:\")\n",
    "    print(results_df.sort_values('AUC', ascending=False))\n",
    "    \n",
    "    return best_result, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result, results_df = test_all_combinations(\n",
    "    num_shadows,\n",
    "    filtered_shadow_preds_in,\n",
    "    filtered_shadow_preds_out,\n",
    "    target_preds,\n",
    "    y_challenge,\n",
    "    y_membership\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
